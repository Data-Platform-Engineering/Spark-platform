name: Upload to S3

on:
  push:
    branches:
      - "**"
    paths:
      - "spark/demo.py"

jobs:
  upload:
    runs-on: ubuntu-latest
    steps:

      - name: checkout git repository
        uses: actions/checkout@v2

      - name: Set up python environment
        uses: actions/setup-python@v2
        with:
          python-version: "3.9.0"

      - name: install dependencies
        run: pip install -r requirements.txt

      - name: Running isort check
        run: isort --check-only  .

      - name: Running flake8 check
        run: flake8 .

      - name: Checkout repo content
        uses: actions/checkout@v4

      - uses: keithweaver/aws-s3-github-action@v1.0.0
        with:
          command: cp
          source: spark/demo.py
          destination: s3://spark-job-data-input/spark_app/demo.py
          aws_access_key_id: ${{ secrets.AWS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws_region: eu-central-1
